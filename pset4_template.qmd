---
title: "problem set IV"
authors: "Jenny Zhong & Summer Negahdar"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

"This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: **SN** **JZ**

 "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
Late coins used this pset: \*\*1 this pset\*\* Late coins left after submission: \*\*\0\*\*
Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
(Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo 
(Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
(Partner 1): tag your submission in Gradescope
 

## Download and explore the Provider of Services (POS) file

1. The variables that I pulled are: 
PRVDR_CTGRY_SBTYP_CD: subtype. Identifies the subtype of the provider, within the primary category. Used in reporting to show the breakdown of provider categories, mainly for hospitals and SNFs.

PRVDR_CTGRY_CD: provider category. Identifies the type of provider participating in the Medicare/Medicaid program.

PGM_TRMNTN_CD: termination code. Indicates the provider’s current termination status.

TRMNTN_EXPRTN_DT: Termination or Expiration Date. Date the provider was terminated. For CLIA providers, date the laboratory's certificate was terminated or the expiration date of the current CLIA certificate.

FAC_NAME: Facility / hospital name. Name of the provider certified to participate in the Medicare and/or Medicaid programs.

ZIP_CD: Zip code. Five-digit ZIP code for a provider's physical address.

CHOW_CNT: CHOW Count. Number of times this provider has changed ownership.

CHOW_DT: CHOW Date. Effective date of the most recent change of ownership for this provider.

CITY_NAME: City. City in which the provider is physically located.

PRVDR_NUM: CMS number/Provider number. Six or ten position identification number that is assigned to a certified provider. This is the CMS Certification Number.

2. 
```{python}
import pandas as pd
import geopandas as gpd
import shapely
import altair as alt
import sys
import pyproj
import os
from pyproj import CRS, Transformer

os.chdir('/Users/jennyzhong/Documents/GitHub/problem-set-4-summer-jenny/')

pos2016 = pd.read_csv('pos2016.csv')

pos2016.head(10)
```

```{python}
#converting to string
pos2016['PRVDR_CTGRY_SBTYP_CD'] = pos2016['PRVDR_CTGRY_SBTYP_CD'].astype(str).str.zfill(2)

pos2016['PRVDR_CTGRY_CD'] = pos2016['PRVDR_CTGRY_CD'].astype(str).str.zfill(2)
pos2016.head()
```

```{python}
st_hospitals2016 = pos2016[(pos2016['PRVDR_CTGRY_SBTYP_CD'] == '1.0') & (pos2016['PRVDR_CTGRY_CD'] == '01')]

print(st_hospitals2016.head())
st_hospitals2016.shape
```

a. 7,245 hospitals are reported in this data. 

b. Compared to sources that provide counts of short-term hospitals in the US, such as the Kaiser Family Foundation, we know that there were nearly 5,000 short term, acute care hospitals in the US in recent years (half of which are rural and half urban). This could be higher than the reported number because of definition differences, for example the dataset could define short-term hospitals different to the Kaiser Family Foundation's definition. 

adding the year 2016
```{python}
st_hospitals2016.loc[:, 'YEAR'] = 2016
print(st_hospitals2016)
```

3. 
for 2017: 
```{python}
#importing pos2017
pos2017 = pd.read_csv("pos2017.csv", encoding='latin1')
```

```{python}
#converting to string
pos2017['PRVDR_CTGRY_SBTYP_CD'] = pos2017['PRVDR_CTGRY_SBTYP_CD'].astype(str).str.zfill(2)

pos2017['PRVDR_CTGRY_CD'] = pos2017['PRVDR_CTGRY_CD'].astype(str).str.zfill(2) ##filling numbers with a zero before integer
pos2017.head()
```

```{python}
#then focusing on short term hospitals 
st_hospitals2017 = pos2017[(pos2017['PRVDR_CTGRY_SBTYP_CD'] == '1.0') & (pos2017['PRVDR_CTGRY_CD'] == '01')]

print(st_hospitals2017.head())
print(st_hospitals2017.tail())
st_hospitals2017.shape
```

Adding the 2017 column 
```{python}
st_hospitals2017.loc[:, 'YEAR'] = 2017

print(st_hospitals2017.head())
```

for 2018: 
```{python}
#importing pos2018
pos2018 = pd.read_csv('pos2018.csv', encoding='latin1')
```

```{python}
#converting to string
pos2018['PRVDR_CTGRY_SBTYP_CD'] = pos2018['PRVDR_CTGRY_SBTYP_CD'].astype(str).str.zfill(2)

pos2018['PRVDR_CTGRY_CD'] = pos2018['PRVDR_CTGRY_CD'].astype(str).str.zfill(2)
pos2018.head(5)
```

```{python}
#then focus on st hospitals 
st_hospitals2018 = pos2018[(pos2018['PRVDR_CTGRY_SBTYP_CD'] == '1.0') & (pos2018['PRVDR_CTGRY_CD'] == '01')]

print(st_hospitals2018.head(5))
print(st_hospitals2018.tail(5))
st_hospitals2018.shape
```

Adding the 2018 column 
```{python}
#column had to be extended
st_hospitals2018.loc[:, 'YEAR'] = 2018

print(st_hospitals2018.head(5))
```


for 2019: 
```{python}
#importing pos2019
pos2019 = pd.read_csv('pos2019.csv', encoding='latin1')

```

```{python}
#converting to string
pos2019['PRVDR_CTGRY_SBTYP_CD'] = pos2019['PRVDR_CTGRY_SBTYP_CD'].astype(str).str.zfill(2)

pos2019['PRVDR_CTGRY_CD'] = pos2019['PRVDR_CTGRY_CD'].astype(str).str.zfill(2)
pos2019.head(5)
```

```{python}
#then focus on st hospitals 
st_hospitals2019 = pos2019[(pos2019['PRVDR_CTGRY_SBTYP_CD'] == '1.0') & (pos2019['PRVDR_CTGRY_CD'] == '01')]

print(st_hospitals2019.head(5))
print(st_hospitals2019.tail(5))
st_hospitals2019.shape
```

Adding the 2019 column 
```{python}
st_hospitals2019.loc[:, 'YEAR'] = 2019

print(st_hospitals2019.head(5))
```

**Appending them together**
```{python}
all_years_data = pd.concat([st_hospitals2016, st_hospitals2017, st_hospitals2018, st_hospitals2019], ignore_index=True)

print(all_years_data.head())
all_years_data.shape
```

**Plotting by observations**
```{python}
observations_by_year = all_years_data.groupby('YEAR').size().reset_index(name='count')

chart13 = alt.Chart(observations_by_year).mark_bar().encode(
    x=alt.X('YEAR:O', title='Year'),
    y=alt.Y('count:Q', title='Number of Observations')  
).properties(
    title='Number of Observations by Year'
)
chart13.display()
```

4. 
    a.
```{python}
unique_hospitals_by_year = all_years_data.groupby('YEAR')['PRVDR_NUM'].nunique().reset_index(name='unique_count')

unique_hospital_chart = alt.Chart(unique_hospitals_by_year).mark_bar().encode(
    x=alt.X('YEAR:O', title='Year'),
    y=alt.Y('unique_count:Q', title='Number of Unique Hospitals') 
).properties(
    title='Number of Unique Hospitals by Year'
)

unique_hospital_chart.display()
```

b. The number of unique hospitals and total observations charts are identical, which means that each hospital has only one record in the dataset for each year. The dataset is structured as a snapshot of unique hospitals, with no intra-year variations or multiple records per hospital within each year. 

## Identify hospital closures in POS file (15 pts) (*)

```{python}
unique_termination_codes = all_years_data['PGM_TRMNTN_CD'].unique()
print(unique_termination_codes)
```

1. 

```{python}
# Ensure PGM_TRMNTN_CD is treated as a string
all_years_data['PGM_TRMNTN_CD'] = all_years_data['PGM_TRMNTN_CD'].astype(str)

# Fill missing values in the PGM_TRMNTN_CD column with '1'
all_years_data['PGM_TRMNTN_CD'] = all_years_data['PGM_TRMNTN_CD'].fillna('1')

# Verify that missing values have been filled
print("Check for missing values in PGM_TRMNTN_CD column after filling:")
print(all_years_data['PGM_TRMNTN_CD'].isnull().sum())  # Should output 0 if no missing values remain

# List to store suspected closures
suspected_closures = []

# Filter for hospitals that were active in 2016
active_2016 = all_years_data[(all_years_data['YEAR'] == 2016) & (all_years_data['PGM_TRMNTN_CD'] == '0')]

# Loop through each hospital active in 2016
for _, hospital in active_2016.iterrows():
    facility_name = hospital['FAC_NAME']
    zip_code = hospital['ZIP_CD']
    
    # Check if the hospital is closed in 2019 with any termination code from ['01', '02', '03', '04']
    closed_2019 = all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2019) &
        (all_years_data['PGM_TRMNTN_CD'].isin(['1', '2', '3', '4']))
    ]
    
    # Retrieve termination codes for 2018 and 2017
    termination_code_2018 = all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2018)
    ]['PGM_TRMNTN_CD'].iloc[0] if not all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2018)
    ].empty else None

    termination_code_2017 = all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2017)
    ]['PGM_TRMNTN_CD'].iloc[0] if not all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) &
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'] == 2017)
    ].empty else None
    
    # If the hospital is closed in 2019, add it to the suspected closures with 2018 and 2017 termination codes
    if not closed_2019.empty:
        suspected_closures.append({
            'FAC_NAME': facility_name,
            'ZIP_CD': zip_code,
            'year_of_closure': 2019,
            'PGM_TRMNTN_CD_2018': termination_code_2018,
            'PGM_TRMNTN_CD_2017': termination_code_2017
        })

# Convert to DataFrame and remove duplicates to ensure unique hospitals
suspected_closures_df = pd.DataFrame(suspected_closures).drop_duplicates(subset=['FAC_NAME', 'ZIP_CD'])

# Display the count of unique suspected closures
num_unique_suspected_closures = len(suspected_closures_df)
print(f"Number of unique hospitals suspected to have closed by 2019: {num_unique_suspected_closures}")
```


2. 

```{python}
print(suspected_closures_df.head(10))
```

3. 
    a.

```{python}
# Lists to store final classifications
properly_closed_hospitals = []
merge_hospitals = []

# Iterate over each hospital in suspected_closures_df
for _, hospital in suspected_closures_df.iterrows():
    facility_name = hospital['FAC_NAME']
    zip_code = hospital['ZIP_CD']
    termination_code_2018 = hospital['PGM_TRMNTN_CD_2018']
    termination_code_2017 = hospital['PGM_TRMNTN_CD_2017']
    
    # Step 1: Check the 2018 termination code
    if termination_code_2018 != '0':
        # If 2018 termination code is not 0, classify as properly closed
        properly_closed_hospitals.append({
            'FAC_NAME': facility_name,
            'ZIP_CD': zip_code,
            'PGM_TRMNTN_CD_2018': termination_code_2018,
            'PGM_TRMNTN_CD_2017': termination_code_2017
        })
    else:
        # Step 2: Check the 2017 termination code if 2018 is 0.0
        if termination_code_2017 == '0':
            # If 2017 is also 0, classify as properly closed
            properly_closed_hospitals.append({
                'FAC_NAME': facility_name,
                'ZIP_CD': zip_code,
                'PGM_TRMNTN_CD_2018': termination_code_2018,
                'PGM_TRMNTN_CD_2017': termination_code_2017
            })
        else:
            # Otherwise, classify as a potential merger
            merge_hospitals.append({
                'FAC_NAME': facility_name,
                'ZIP_CD': zip_code,
                'PGM_TRMNTN_CD_2018': termination_code_2018,
                'PGM_TRMNTN_CD_2017': termination_code_2017
            })

# Convert results to DataFrames for inspection
properly_closed_hospitals_df = pd.DataFrame(properly_closed_hospitals)
merge_hospitals_df = pd.DataFrame(merge_hospitals)

# Display results
print("Properly closed hospitals:")
print(properly_closed_hospitals_df)

print("\nHospitals suspected of merger:")
print(merge_hospitals_df)

# Number of properly closed hospitals
num_properly_closed = properly_closed_hospitals_df.shape['0']
print(f"Number of properly closed hospitals: {num_properly_closed}")

# Number of suspected mergers
num_suspected_mergers = merge_hospitals_df.shape['0']
print(f"Number of suspected mergers: {num_suspected_mergers}")
```


```{python}
# Initialize lists to store properly closed hospitals and suspected mergers
properly_closed_hospitals = []
merge_hospitals = []

# Iterate over each hospital in suspected_closures_df
for _, hospital in suspected_closures_df.iterrows():
    facility_name = hospital['FAC_NAME']
    zip_code = hospital['ZIP_CD']
    
    # Filter for this hospital's data in all_years_data for the years 2017 and 2018
    hospital_data = all_years_data[
        (all_years_data['FAC_NAME'] == facility_name) & 
        (all_years_data['ZIP_CD'] == zip_code) &
        (all_years_data['YEAR'].isin([2017, 2018]))
    ]

    # Get the termination codes for 2018 and 2017, defaulting to "1" if the year is missing
    termination_2018 = hospital_data[hospital_data['YEAR'] == 2018]['PGM_TRMNTN_CD'].iloc[0] if not hospital_data[hospital_data['YEAR'] == 2018].empty else '1'
    termination_2017 = hospital_data[hospital_data['YEAR'] == 2017]['PGM_TRMNTN_CD'].iloc[0] if not hospital_data[hospital_data['YEAR'] == 2017].empty else '1'

    # Classification logic as per the description
    if termination_2018 != '0':
        # Properly closed if terminated in 2018 (code other than "0")
        properly_closed_hospitals.append({
            'FAC_NAME': facility_name,
            'ZIP_CD': zip_code,
            'PGM_TRMNTN_CD_2017': termination_2017,
            'PGM_TRMNTN_CD_2018': termination_2018
        })
    elif termination_2018 == '0':
        # If active in 2018, check 2017 status
        if termination_2017 == '0':
            # Properly closed if also active in 2017
            properly_closed_hospitals.append({
                'FAC_NAME': facility_name,
                'ZIP_CD': zip_code,
                'PGM_TRMNTN_CD_2017': termination_2017,
                'PGM_TRMNTN_CD_2018': termination_2018
            })
        else:
            # Otherwise, add to suspected mergers
            merge_hospitals.append({
                'FAC_NAME': facility_name,
                'ZIP_CD': zip_code,
                'PGM_TRMNTN_CD_2017': termination_2017,
                'PGM_TRMNTN_CD_2018': termination_2018
            })

# Convert results to DataFrames for inspection
properly_closed_hospitals_df = pd.DataFrame(properly_closed_hospitals)
merge_hospitals_df = pd.DataFrame(merge_hospitals)
# Number of properly closed hospitals
num_properly_closed = properly_closed_hospitals_df.shape[0]
print(f"Number of properly closed hospitals: {num_properly_closed}")

# Number of suspected mergers
num_suspected_mergers = merge_hospitals_df.shape[0]
print(f"Number of suspected mergers: {num_suspected_mergers}")
```

There are no potential merged hospitals!


    a. there are no merged hospitals!
    b.

 ```{python}
    # Sort the properly closed hospitals by FAC_NAME and display the first 10 rows
sorted_properly_closed_hospitals = properly_closed_hospitals_df.sort_values(by='FAC_NAME').head(10)

# Display the sorted list
print("First 10 corrected hospital closures sorted by name:")
print(sorted_properly_closed_hospitals)
```

## Download Census zip code shapefile (10 pt) 

1. 
    a. There are five types of file types here. 
**.shp (shape file)**: file contains the geometric shapes. It stores the spatial coordinates and shapes of objects like points, lines or polygons. 
**.shx (shape index format)**: file is an index of the geometry in the .shp file, and provides quick access to the geometric shapes. 
**.dbf (attribute format)**: file contains attribute data in tabular format, with each row corresponding to a feature and each column containing attributes associated with that feature. 
**.prj (projection format)**: file contains information about the coordinate sysetm and projection, and defines how shapes are mapped onto Earth's surface. 
**.xml (metadata format)**: file contains metadata, contains descriptive information about dataset such as source, creation date, projection dates. 

    b. 
| File Extension | Size (MB)  |
|----------------|------------|
| .shp           | 837.5      |
| .dbf           | 6.4        |
| .prj           | 0.000165   |
| .shx           | 0.000265   |
| .xml           | 0.016      |

2. 

```{python}
import geopandas as gpd
```

2. 
```{python}
shapefile_path = './gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp'

try:
    zip_shapes = gpd.read_file(shapefile_path)
    print("Shapefile loaded successfully with SHX restoration.")
except Exception as e:
    print("Error loading shapefile:", e)
```

```{python}
#filtering for texas zip codes
texas_zip_shapes = zip_shapes[zip_shapes['ZCTA5'].astype(str).str.startswith(('75', '76', '77', '78', '79')) | (zip_shapes['ZCTA5'] == '733')]

print(texas_zip_shapes.head())
print("Number of Texas ZIP codes:", texas_zip_shapes.shape[0])
```

```{python}
#pos2016 
pos2016['ZIP_CD'] = pos2016['ZIP_CD'].astype(str)

texas_hospitals_2016 = pos2016[pos2016['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79')) | (pos2016['ZIP_CD'] == '733')]

print(texas_hospitals_2016.head())
print("Number of Texas hospital records in 2016:", texas_hospitals_2016.shape[0])
```

```{python}
#counting the number of hospitals per ZIP code:
hospitals_per_zip = texas_hospitals_2016.groupby('ZIP_CD').size().reset_index(name='hospital_count')
```

```{python}
#merging for chloropleth mapping
texas_zip_shapes.loc[:, 'ZCTA5'] = texas_zip_shapes['ZCTA5'].astype(str)
hospitals_per_zip['ZIP_CD'] = hospitals_per_zip['ZIP_CD'].astype(str)

# convert ZIP_CD to integer, then to string with zero-padding to ensure 5-digit format
hospitals_per_zip['ZIP_CD'] = hospitals_per_zip['ZIP_CD'].astype(float).astype(int).astype(str).str.zfill(5)

# re-check the formatting
print("Formatted ZIP codes in hospitals_per_zip:", hospitals_per_zip['ZIP_CD'].unique()[:10])

# now, re-run the merge
texas_zip_shapes = texas_zip_shapes.merge(hospitals_per_zip, left_on='ZCTA5', right_on='ZIP_CD', how='left')

# fill NaN values in 'hospital_count' with 0 for ZIP codes with no hospitals
texas_zip_shapes['hospital_count'] = texas_zip_shapes['hospital_count'].fillna(0)

# Verify the number of common ZIP codes and non-zero hospital counts
print("Number of ZIP codes with hospital_count as 0:", (texas_zip_shapes['hospital_count'] == 0).sum())
print("Total ZIP codes in Texas:", texas_zip_shapes.shape[0])
print(texas_zip_shapes[['ZCTA5', 'hospital_count']].head(10))

```

```{python}
# Adjust the merge by specifying suffixes to handle duplicate columns
texas_zip_shapes = texas_zip_shapes.merge(
    hospitals_per_zip, 
    left_on='ZCTA5', 
    right_on='ZIP_CD', 
    how='left', 
    suffixes=('', '_y')  # Prevents '_x' and '_y' conflicts
)

# Drop any unnecessary duplicate columns that may arise
texas_zip_shapes = texas_zip_shapes.drop(columns=['ZIP_CD_y'], errors='ignore')

# Fill NaN values in 'hospital_count' with 0 for ZIP codes with no hospitals
texas_zip_shapes['hospital_count'] = texas_zip_shapes['hospital_count'].fillna(0)

# Verify the result
print("Number of ZIP codes with hospital_count as 0:", (texas_zip_shapes['hospital_count'] == 0).sum())
print("Total ZIP codes in Texas:", texas_zip_shapes.shape[0])
print(texas_zip_shapes[['ZCTA5', 'hospital_count']].head(10))
```

```{python}

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
texas_zip_shapes.plot(column='hospital_count', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Number of Hospitals per ZIP Code in Texas (2016)")
ax.set_axis_off()
plt.show()
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)


1. 
```{python}
#I will start by creating centeroids:
zip_shapes['centroids']= zip_shapes.geometry.centroid
#this will change the default geometry to centroids
all_zip_centroid= zip_shapes.copy().set_geometry('centroids')
print(all_zip_centroid.shape)
print(all_zip_centroid.columns)
print(all_zip_centroid.head(10))
```

GEO_ID: it is ta code to identify the grographic type: state, county, zipcode

ZCTA5:it is 5 digit zip code tabulation area(approximation of postal codes)

NAME: is the same as ZCTA5 only more user friendly. 

LSDA: legal/statistical area description categorizing geo area type( state, county,zipcode(in this case our description is zip code ZCTA5))

CENSUSAREA:The land area of the ZCTA in square miles as calculated by the Census Bureau. It measures the physical size of each area, excluding water bodies.


2. 
```{python}
from shapely.ops import unary_union

# Step 1: Filter for Texas ZIP codes (starting with 75, 76, 77, 78, or 79)
zips_texas_centroids = all_zip_centroid[all_zip_centroid['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

# Step 2: Filter for Texas and bordering states’ ZIP codes
zips_texas_borderstates_centroids = all_zip_centroid[all_zip_centroid['ZCTA5'].str.startswith(
    ('75', '76', '77', '78', '79', '70', '71', '72', '73', '74', '80', '81', '88', '87', '86')
)]

# Step 3: Count unique ZIP codes in each subset
num_texas_zips = zips_texas_centroids['ZCTA5'].nunique()
num_bordering_zips = zips_texas_borderstates_centroids['ZCTA5'].nunique()

print(f"Number of unique Texas ZIP codes: {num_texas_zips}")
print(f"Number of unique ZIP codes in Texas and bordering states: {num_bordering_zips}")

# Step 4: Create a function to check intersection with Texas polygon
def intersects_texas(texas_polygon, other_polygon):
    return texas_polygon.intersects(other_polygon)

# Step 5: Combine all Texas ZIP codes into a single polygon
texas_polygon = unary_union(zips_texas_centroids.geometry)

# Step 6: Apply the intersection function to find ZIP codes that are in bordering states
zips_texas_borderstates_centroids['borders_texas'] = zips_texas_borderstates_centroids.geometry.apply(
    lambda geom: intersects_texas(texas_polygon, geom)
)

# Count unique ZIP codes in subsets
unique_texas_zips = zips_texas_centroids['ZCTA5'].nunique()
unique_bordering_zips = zips_texas_borderstates_centroids[zips_texas_borderstates_centroids['borders_texas']]['ZCTA5'].nunique()

print(f"Unique Texas ZIP codes: {unique_texas_zips}")
print(f"Unique ZIP codes in Texas and bordering states: {unique_bordering_zips}")
```

3. 

```{python}
## the ZIP_CD column in our properly_closed_df is integer, I have to convert it!
# Ensure ZIP_CD in hospitals_per_zip is converted to strings without decimals
texas_hospitals_2016 = pos2016[pos2016['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79')) | (pos2016['ZIP_CD'] == '733')]
hospitals_per_zip = texas_hospitals_2016.groupby('ZIP_CD').size().reset_index(name='hospital_count')
hospitals_per_zip['ZIP_CD'] = hospitals_per_zip['ZIP_CD'].astype(str).str.zfill(5)

# Also ensure ZIP codes in zips_texas_borderstates_centroids are strings with zero-padding
zips_texas_borderstates_centroids[zip_column] = zips_texas_borderstates_centroids[zip_column].astype(str).str.zfill(5)

# Verify the cleaned ZIP_CD column
print("Sample ZIP codes in hospitals_per_zip (should be string with zero-padding):")
print(hospitals_per_zip['ZIP_CD'].head())

print("Sample ZIP codes in zips_texas_borderstates_centroids (should be string with zero-padding):")
print(zips_texas_borderstates_centroids[zip_column].head())

# Proceed with merging and filtering
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospitals_per_zip,
    left_on='ZCTA5',
    right_on='ZIP_CD',
    how='inner'
)

# Filter for ZIP codes with at least one hospital
zips_withhospital_centroids = zips_withhospital_centroids[zips_withhospital_centroids['hospital_count'] > 0]

# Display the final result and count unique ZIP codes
print("zips_withhospital_centroids with at least 1 hospital:")
print(zips_withhospital_centroids.head())
num_unique_zip_codes = zips_withhospital_centroids[zip_column].nunique()
print(f"Number of unique ZIP codes with at least 1 hospital in 2016: {num_unique_zip_codes}")
```

I inner merged on zip code but had osme problems because they did not match (zip code in geo file had decimals and had to be converted)
4. 


    a.

```{python}
import numpy as np
import time
start_time_T= time.time()
# Conversion factors
LAT_TO_MILES = 69.0  # Approximate miles per degree latitude
LON_TO_MILES = 55.0  # Approximate miles per degree longitude in Texas

# Function to calculate approximate distance in miles between two points given in degrees
def degree_to_miles_distance(point1, point2):
    lat_diff = (point1.y - point2.y) * LAT_TO_MILES
    lon_diff = (point1.x - point2.x) * LON_TO_MILES
    return np.sqrt(lat_diff**2 + lon_diff**2)

# I am subsetting the df with the first 10 
Q4a_subset = zips_texas_centroids.head(10)
# Step 2: Time the nearest distance calculation
start_time = time.time()

# Calculating the nearest distances for the subset
Q4a_subset['nearest_hospital_distance'] = Q4a_subset['centroids'].apply(
    lambda x: calculate_nearest_distance(x, zips_withhospital_centroids['centroids'])
)

end_time = time.time()

# Step 3: Calculate time taken and estimate total time
subset_duration = end_time - start_time
total_zips_count = len(zips_texas_centroids)
estimated_total_duration = (subset_duration / 10) * total_zips_count

# Display the results
print(f"Time taken for 10 ZIP codes: {subset_duration:.2f} seconds")
print(f"Estimated time for entire dataset: {estimated_total_duration / 60:.2f} minutes")
```

It will take approximatelty 18 seconds to run the whole dataset!
    b.

look at part C where I calculated the actual distance. I have mentioned how much it will take to run the whole thing there. it is 36 seconds!


    c.
the unit is degree, which is approximately 69 miles or 111 km. here is convertion:

```{python}
start_time_T= time.time()

# Apply this function to each Texas ZIP code centroid and find the nearest hospital distance
zips_texas_centroids['nearest_hospital_distance_miles'] = zips_texas_centroids['centroids'].apply(
    lambda centroid: min(
        degree_to_miles_distance(centroid, hospital_centroid) 
        for hospital_centroid in zips_withhospital_centroids['centroids']
    )
)

# Calculate the average distance to the nearest hospital in miles
average_distance_miles = zips_texas_centroids['nearest_hospital_distance_miles'].mean()
end_time_T=time.time()
total_time= end_time_T - start_time_T
print(f"The average distance to the nearest hospital in Texas (in miles) is: {average_distance_miles:.2f}", "miles")
print("the total time required to calculate the prompt above is", total_time, "seconds")
```

5. 

```{python}

# Step 1: Calculate the average distance to the nearest hospital in miles
average_distance = zips_texas_centroids['nearest_hospital_distance_miles'].mean()
print(f"Average distance to the nearest hospital for each ZIP code in Texas: {average_distance:.2f} miles")

# Step 2: Map the nearest hospital distance for each ZIP code in Texas
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
zips_texas_centroids.plot(column='nearest_hospital_distance_miles', cmap='YlOrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Distance to the Nearest Hospital for Each ZIP Code in Texas (Miles)")
ax.set_axis_off()
plt.show()
```

    
## Effects of closures on access in Texas (15 pts)
1. There are 20 hospitals that have had at least one closure between 2016-2019.
```{python}
corrected_closures_df = pd.DataFrame(properly_closed_hospitals)

corrected_closures_df['ZIP_CD'] = corrected_closures_df['ZIP_CD'].astype(str)
```

```{python}
texas_closures = corrected_closures_df[corrected_closures_df['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79', '733'))]
```

```{python}
closures_by_zip = texas_closures.groupby('ZIP_CD').size().reset_index(name='closure_count')
```

```{python}
print("Number of hospital closures by ZIP code in Texas (2016-2019):")
print(closures_by_zip)
```

2.  
```{python}
print(texas_zip_shapes.columns)
print(corrected_closures_df.columns)
```

```{python}
closures_by_zip = corrected_closures_df.groupby('ZIP_CD').size().reset_index(name='closure_count')

texas_zip_shapes = texas_zip_shapes.drop(columns=['closure_count_x', 'closure_count_y'], errors='ignore')

texas_zip_shapes = texas_zip_shapes.merge(closures_by_zip, on='ZIP_CD', how='left', suffixes=('', '_closure'))

texas_zip_shapes['closure_count'] = texas_zip_shapes['closure_count'].fillna(0)

print(texas_zip_shapes.columns)
print(texas_zip_shapes[['ZIP_CD', 'closure_count']].head())
```

```{python}
print(len(texas_zip_shapes))
```

3. 
```{python}
affected_zip_codes = affected_zip_codes[affected_zip_codes['geometry'].notnull()]

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
affected_zip_codes.plot(column='closure_count', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Texas ZIP Codes Directly Affected by Hospital Closures (2016-2019)")
ax.set_axis_off()
plt.show()
```

4. 
```{python}
texas_zip_shapes['affected_status'] = 'Not Affected' 
directly_affected = texas_zip_shapes[texas_zip_shapes['closure_count'] > 0]
texas_zip_shapes.loc[texas_zip_shapes['closure_count'] > 0, 'affected_status'] = 'Directly Affected'

texas_zip_shapes = texas_zip_shapes.to_crs(epsg=5070)
directly_affected = directly_affected.to_crs(epsg=5070)

buffer_10_miles = directly_affected.buffer(16093.4).unary_union 

within_10_miles = texas_zip_shapes[texas_zip_shapes['geometry'].intersects(buffer_10_miles) & (texas_zip_shapes['affected_status'] == 'Not Affected')]
texas_zip_shapes.loc[within_10_miles.index, 'affected_status'] = 'Within 10 Miles of Closure'

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
texas_zip_shapes.plot(column='affected_status', cmap='Set1', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Texas ZIP Codes by Closure Impact (2016-2019)")
ax.set_axis_off()
plt.show()
```

## Reflecting on the exercise (10 pts) 
Partner 1: 
The “first-pass” method for identifying hospital closures has limitations that could lead to inaccuracies. Capturing data only annually risks misidentifying closures, as hospitals that close and reopen within a year may still appear as closed. More frequent data collection (e.g., monthly) and cross-referencing with reliable healthcare databases like CMS or Medicare could improve accuracy. Additionally, hospitals that merge or reclassify, such as converting to outpatient centers, may seem closed but still offer services; tracking such changes would help clarify true closures. Finally, factoring in geographical and demographic differences, especially between urban and rural areas, would provide a more realistic view of how closures impact access, better reflecting actual community needs.

Partner 2: 
Identifying affected ZIP codes by proximity to closures is helpful but might not fully reflect changes in hospital accessibility. ZIP codes vary in size and infrastructure, making it overly simplistic to define access by ZIP code alone. Using real travel distances or public transit times would better capture access limitations. Additionally, hospital closures impact communities differently depending on the services provided—an emergency center’s closure has a greater effect on access than a specialized facility. Finally, rural areas often rely on a single hospital, unlike urban areas with multiple options within a short distance. Adjusting thresholds for rural and urban settings could improve the measure’s accuracy.

